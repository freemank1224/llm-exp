# ğŸ³ Docker éƒ¨ç½²ä½¿ç”¨æŒ‡å—

æœ¬é¡¹ç›®æä¾›äº†å®Œæ•´çš„Dockerè§£å†³æ–¹æ¡ˆï¼Œè®©æ‚¨æ— éœ€é…ç½®å¤æ‚çš„Pythonç¯å¢ƒå³å¯è¿è¡ŒLLMé¢„æµ‹åº”ç”¨ã€‚

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šä½¿ç”¨é¢„æ„å»ºé•œåƒï¼ˆæ¨èï¼‰

æˆ‘ä»¬å·²ç»ä¸ºæ‚¨æ„å»ºå¥½äº†åŒ…å«æ‰€æœ‰ä¾èµ–å’Œæ¨¡å‹çš„Dockeré•œåƒï¼Œæ‚¨åªéœ€ä¸€æ¡å‘½ä»¤å³å¯è¿è¡Œï¼š

```bash
# æ‹‰å–å¹¶è¿è¡Œé•œåƒ
docker run -d -p 8501:8501 --name llm-prediction ghcr.io/YOUR-USERNAME/llm-exp:latest

# è®¿é—®åº”ç”¨
open http://localhost:8501
```

### æ–¹å¼äºŒï¼šæœ¬åœ°æ„å»ºé•œåƒ

å¦‚æœæ‚¨æƒ³è‡ªå·±æ„å»ºé•œåƒæˆ–è€…éœ€è¦ä¿®æ”¹ä»£ç ï¼š

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/YOUR-USERNAME/llm-exp.git
cd llm-exp

# 2. æ„å»ºé•œåƒï¼ˆè‡ªåŠ¨åŒ–è„šæœ¬ï¼‰
./build_docker.sh

# 3. è¿è¡Œå®¹å™¨ï¼ˆè‡ªåŠ¨åŒ–è„šæœ¬ï¼‰
./run_docker.sh
```

## ğŸ“‹ è¯¦ç»†ä½¿ç”¨è¯´æ˜

### ç³»ç»Ÿè¦æ±‚

- Docker Desktop (Windows/macOS) æˆ– Docker Engine (Linux)
- è‡³å°‘ 8GB å¯ç”¨ç£ç›˜ç©ºé—´
- è‡³å°‘ 4GB å¯ç”¨å†…å­˜

### ç«¯å£é…ç½®

- é»˜è®¤ç«¯å£ï¼š`8501`
- è®¿é—®åœ°å€ï¼š`http://localhost:8501`
- è‡ªå®šä¹‰ç«¯å£ï¼š`docker run -p YOUR_PORT:8501 ...`

### å®¹å™¨ç®¡ç†å‘½ä»¤

```bash
# æŸ¥çœ‹è¿è¡ŒçŠ¶æ€
docker ps

# æŸ¥çœ‹æ—¥å¿—
docker logs llm-prediction

# å®æ—¶æŸ¥çœ‹æ—¥å¿—
docker logs -f llm-prediction

# åœæ­¢å®¹å™¨
docker stop llm-prediction

# é‡å¯å®¹å™¨
docker restart llm-prediction

# åˆ é™¤å®¹å™¨
docker rm llm-prediction

# åˆ é™¤é•œåƒ
docker rmi ghcr.io/YOUR-USERNAME/llm-exp:latest
```

## ğŸ”§ é«˜çº§é…ç½®

### ç¯å¢ƒå˜é‡

æ‚¨å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è‡ªå®šä¹‰é…ç½®ï¼š

```bash
docker run -d \
  -p 8501:8501 \
  -e STREAMLIT_SERVER_PORT=8501 \
  -e STREAMLIT_SERVER_ADDRESS=0.0.0.0 \
  --name llm-prediction \
  ghcr.io/YOUR-USERNAME/llm-exp:latest
```

### æ•°æ®æŒä¹…åŒ–

å¦‚æœæ‚¨éœ€è¦æŒä¹…åŒ–æŸäº›æ•°æ®ï¼š

```bash
docker run -d \
  -p 8501:8501 \
  -v $(pwd)/data:/app/data \
  --name llm-prediction \
  ghcr.io/YOUR-USERNAME/llm-exp:latest
```

### èµ„æºé™åˆ¶

ä¸ºå®¹å™¨è®¾ç½®èµ„æºé™åˆ¶ï¼š

```bash
docker run -d \
  -p 8501:8501 \
  --memory=4g \
  --cpus=2 \
  --name llm-prediction \
  ghcr.io/YOUR-USERNAME/llm-exp:latest
```

## ğŸš€ ç”Ÿäº§éƒ¨ç½²

### ä½¿ç”¨ Docker Compose

åˆ›å»º `docker-compose.yml`ï¼š

```yaml
version: '3.8'
services:
  llm-prediction:
    image: ghcr.io/YOUR-USERNAME/llm-exp:latest
    ports:
      - "8501:8501"
    restart: unless-stopped
    environment:
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
```

è¿è¡Œï¼š
```bash
docker-compose up -d
```

### åå‘ä»£ç†é…ç½®

å¦‚æœæ‚¨ä½¿ç”¨ Nginx ä½œä¸ºåå‘ä»£ç†ï¼š

```nginx
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://localhost:8501;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

## â“ å¸¸è§é—®é¢˜

### Q: é•œåƒå¤ªå¤§æ€ä¹ˆåŠï¼Ÿ
A: æˆ‘ä»¬çš„é•œåƒåŒ…å«äº†å®Œæ•´çš„LLMæ¨¡å‹ï¼ˆçº¦3GBï¼‰ï¼Œè¿™æ˜¯ä¸ºäº†ç¡®ä¿ç¦»çº¿è¿è¡Œã€‚å¦‚æœå­˜å‚¨ç©ºé—´æœ‰é™ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ç‰ˆæœ¬ã€‚

### Q: å®¹å™¨å¯åŠ¨å¤±è´¥ï¼Ÿ
A: è¯·æ£€æŸ¥ï¼š
1. Dockeræ˜¯å¦æ­£å¸¸è¿è¡Œ
2. ç«¯å£8501æ˜¯å¦è¢«å ç”¨
3. ç³»ç»Ÿå†…å­˜æ˜¯å¦å……è¶³ï¼ˆå»ºè®®4GB+ï¼‰

### Q: å¦‚ä½•æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬ï¼Ÿ
A: 
```bash
# åœæ­¢å¹¶åˆ é™¤æ—§å®¹å™¨
docker stop llm-prediction && docker rm llm-prediction

# æ‹‰å–æœ€æ–°é•œåƒ
docker pull ghcr.io/YOUR-USERNAME/llm-exp:latest

# è¿è¡Œæ–°å®¹å™¨
docker run -d -p 8501:8501 --name llm-prediction ghcr.io/YOUR-USERNAME/llm-exp:latest
```

### Q: å¦‚ä½•æŸ¥çœ‹å®¹å™¨å†…éƒ¨ï¼Ÿ
A: 
```bash
# è¿›å…¥å®¹å™¨shell
docker exec -it llm-prediction /bin/bash

# æŸ¥çœ‹æ–‡ä»¶
docker exec llm-prediction ls -la /app
```

## ğŸ› ï¸ å¼€å‘è€…ä¿¡æ¯

### æ„å»ºä¿¡æ¯
- åŸºç¡€é•œåƒï¼š`python:3.10-slim`
- æ¨¡å‹ï¼šQwen2-1.5B
- æ¡†æ¶ï¼šStreamlit
- ç¼“å­˜ç›®å½•ï¼š`/app/models`

### è‡ªå®šä¹‰æ„å»º
å¦‚æœæ‚¨éœ€è¦ä¿®æ”¹æ¨¡å‹æˆ–é…ç½®ï¼Œå¯ä»¥ç¼–è¾‘ä»¥ä¸‹æ–‡ä»¶ï¼š
- `Dockerfile`ï¼šé•œåƒæ„å»ºé…ç½®
- `download_models.py`ï¼šæ¨¡å‹ä¸‹è½½è„šæœ¬
- `requirements.txt`ï¼šPythonä¾èµ–

## ğŸ“ æ”¯æŒ

å¦‚æœæ‚¨åœ¨ä½¿ç”¨Dockeréƒ¨ç½²æ—¶é‡åˆ°é—®é¢˜ï¼š

1. æŸ¥çœ‹é¡¹ç›® [Issues](https://github.com/YOUR-USERNAME/llm-exp/issues)
2. æäº¤æ–°çš„ Issue å¹¶åŒ…å«ï¼š
   - æ“ä½œç³»ç»Ÿä¿¡æ¯
   - Dockerç‰ˆæœ¬
   - é”™è¯¯æ—¥å¿—
   - é‡ç°æ­¥éª¤

---

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒé¡¹ç›®ä¸» [README.md](README.md)ã€‚

# ã€Œç¥å¥‡çš„çŒœè¯å°èƒ½æ‰‹ã€â€”â€” ç»™å­©å­çš„äº’åŠ¨å¼å¤§è¯­è¨€æ¨¡å‹ç§‘æ™®èŠ‚ç›®

è¿™æ˜¯ä¸€ä¸ªä¸ºæ²¡æœ‰AIåŸºç¡€çŸ¥è¯†çš„å°å­¦/ä¸­å­¦ç”Ÿè®²è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„æµ‹åŸç†çš„äº’åŠ¨æ¼”ç¤ºé¡¹ç›®ã€‚æ—¨åœ¨é€šè¿‡äº¤äº’å¼é¡µé¢å’Œæ¼”ç¤ºæ¥è§£é‡ŠLLMçš„å·¥ä½œåŸç†ã€‚é‡‡ç”¨æ­¤é¡¹ç›®ï¼Œèƒ½å¤Ÿè¿œè¶…PPTçš„å®æ—¶äº’åŠ¨æ¼”ç¤ºæ•ˆæœï¼Œç”ŸåŠ¨çš„è®²è¿°LLMçš„ã€Œä¸‹ä¸€ä¸ªè¯å…ƒé¢„æµ‹ã€çš„å·¥ä½œæœºåˆ¶ã€‚

## å®‰è£…è¯´æ˜

### ä½¿ç”¨ Conda è¿›è¡Œç¯å¢ƒç®¡ç†

#### macOS ç”¨æˆ·
1. å®‰è£… Miniconda æˆ– Anacondaï¼š
   è®¿é—® [Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/macos.html) æˆ– [Anaconda](https://www.anaconda.com/products/distribution) ä¸‹è½½é¡µé¢ï¼Œä¸‹è½½å¹¶æŒ‰ç…§è¯´æ˜å®‰è£…ã€‚
2. åˆ›å»ºæ–°ç¯å¢ƒå¹¶æ¿€æ´»ï¼š
   ```bash
   conda create --name llm-prediction python=3.9
   conda activate llm-prediction
   ```
3. å®‰è£…ä¾èµ–é¡¹ï¼š
   ```bash
   pip install -r requirements.txt
   ```

#### Windows ç”¨æˆ·
1. å®‰è£… Miniconda æˆ– Anacondaï¼š
   è®¿é—® [Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/windows.html) æˆ– [Anaconda](https://www.anaconda.com/products/distribution) ä¸‹è½½é¡µé¢ï¼Œä¸‹è½½å¹¶æŒ‰ç…§è¯´æ˜å®‰è£…ã€‚
2. åˆ›å»ºæ–°ç¯å¢ƒå¹¶æ¿€æ´»ï¼š
   ```bash
   conda create --name llm-prediction python=3.9
   conda activate llm-prediction
   ```
3. å®‰è£…ä¾èµ–é¡¹ï¼š
   ```bash
   pip install -r requirements_win.txt
   ```

## è¿è¡Œé¡¹ç›®

### æ–¹å¼ä¸€ï¼šä½¿ç”¨Dockerï¼ˆæ¨èï¼Œä¸€é”®è¿è¡Œï¼‰

#### ğŸš€ ç›´æ¥ä½¿ç”¨é¢„æ„å»ºé•œåƒ
```bash
# æ‹‰å–é¢„æ„å»ºçš„é•œåƒï¼ˆåŒ…å«æ¨¡å‹ï¼‰
docker pull ghcr.io/freemank1224/llm-exp:latest

# è¿è¡Œå®¹å™¨
docker run -d -p 8501:8501 --name llm-prediction ghcr.io/freemank1224/llm-exp:latest

# è®¿é—®åº”ç”¨ï¼šhttp://localhost:8501
```

#### ğŸ”¨ æœ¬åœ°æ„å»ºé•œåƒ
å¦‚æœæ‚¨æƒ³è‡ªå·±æ„å»ºé•œåƒï¼š
```bash
# å…‹éš†ä»“åº“
git clone https://github.com/freemank1224/llm-exp.git
cd llm-exp

# æ„å»ºé•œåƒï¼ˆä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹åˆ°é•œåƒä¸­ï¼‰
./build_docker.sh

# è¿è¡Œå®¹å™¨
./run_docker.sh
```

**Dockerä¼˜åŠ¿ï¼š**
- âœ… æ— éœ€å®‰è£…Pythonç¯å¢ƒå’Œä¾èµ–
- âœ… æ¨¡å‹å·²é¢„ä¸‹è½½ï¼Œå¯åŠ¨å³å¯ä½¿ç”¨
- âœ… è·¨å¹³å°å…¼å®¹ï¼ˆWindows/macOS/Linuxï¼‰
- âœ… ä¸€é”®éƒ¨ç½²ï¼Œé¿å…ç¯å¢ƒé—®é¢˜

### æ–¹å¼äºŒï¼šæœ¬åœ°Pythonç¯å¢ƒè¿è¡Œ

æ¿€æ´»ç¯å¢ƒåï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨åº”ç”¨ç¨‹åºï¼š
```bash
streamlit run Home.py
```


---
# ã€ŒMagic Token Predictorã€: An interactive course of LLM prediction princeple for kids

This is an interactive demonstration project designed to explain the principles of Large Language Models (LLMs) to primary/secondary school students who have no AI background knowledge. It aims to explain how LLMs work through interactive pages and demonstrations. By utilizing this project, we can achieve real-time interactive demonstration effects far beyond PowerPoint presentations, vividly illustrating the "next token prediction" mechanism of LLMs.

## Installation Instructions

### Using Conda for Environment Management

#### macOS Users
1. Install Miniconda or Anaconda:
   Visit the [Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/macos.html) or [Anaconda](https://www.anaconda.com/products/distribution) download page, download, and follow the instructions to install.
2. Create and activate a new environment:
   ```bash
   conda create --name llm-prediction python=3.9
   conda activate llm-prediction
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

#### Windows Users
1. Install Miniconda or Anaconda:
   Visit the [Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/windows.html) or [Anaconda](https://www.anaconda.com/products/distribution) download page, download, and follow the instructions to install.
2. Create and activate a new environment:
   ```bash
   conda create --name llm-prediction python=3.9
   conda activate llm-prediction
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements_win.txt
   ```

## Running the Project

### Method 1: Using Docker (Recommended, One-Click Run)

#### ğŸš€ Use Pre-built Image
```bash
# Pull the pre-built image (with models included)
docker pull ghcr.io/freemank1224/llm-exp:latest

# Run container
docker run -d -p 8501:8501 --name llm-prediction ghcr.io/freemank1224/llm-exp:latest

# Access the app: http://localhost:8501
```

#### ğŸ”¨ Build Image Locally
If you want to build the image yourself:
```bash
# Clone repository
git clone https://github.com/freemank1224/llm-exp.git
cd llm-exp

# Build image (automatically downloads models into the image)
./build_docker.sh

# Run container
./run_docker.sh
```

**Docker Advantages:**
- âœ… No need to install Python environment and dependencies
- âœ… Models are pre-downloaded, ready to use on startup
- âœ… Cross-platform compatibility (Windows/macOS/Linux)
- âœ… One-click deployment, avoiding environment issues

### Method 2: Local Python Environment

After activating the environment, run the following command to start the application:
```bash
streamlit run Home.py
